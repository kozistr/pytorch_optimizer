## Change Log

### Feature

* Implement AdaMax optimizer (#148)
  * A variant of Adam based on the infinity norm
* Implement Gravity optimizer (#151)
  * [a Kinematic Approach on Optimization in Deep Learning](https://arxiv.org/abs/2101.09192)
* Implement AdaSmooth optimizer (#153)
  * [An Adaptive Learning Rate Method based on Effective Ratio](https://arxiv.org/abs/2204.00825v1)
* Implement SRMM optimizer (#154)
  * [Stochastic regularized majorization-minimization with weakly convex and multi-convex surrogates](https://arxiv.org/abs/2201.01652)
* Implement AvaGrad optimizer (#155) 
  * [Domain-independent Dominance of Adaptive Methods](https://arxiv.org/abs/1912.01823)
* Implement AdaShift optimizer (#157) 
  * [Decorrelation and Convergence of Adaptive Learning Rate Methods](https://arxiv.org/abs/1810.00143v4)
* Upgrade to D-Adaptation v3 (#158, #159)
* Implement AdaDelta optimizer (#160)
  * [An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701v1) 

### Docs

* Fix readthedocs build issue (#156)
* Move citations into table (#156) 

### Refactor

* Refactor validation logic (#149, #150)
* Rename `amsbound`, `amsgrad` terms into `ams_bound` (#149)
* Return gradient instead of the parameter, AGC. (#149)
* Refactor duplicates (e.g. rectified step size, AMSBound, AdamD, AdaNorm, weight decay) into re-usable functions (#150)
* Move `pytorch_optimizer.experimental` under `pytorch_optimizer.*.experimental`

### Diff

[2.8.0...2.9.0](https://github.com/kozistr/pytorch_optimizer/compare/v2.8.0...v2.9.0)
