## Change Log

### Feature

* Support the `foreach` functionality. (#287, #476)
  * Currently, 10 optimizers (e.g. StableAdamW, Lion, AdaBelief, ...) support `foreach`.
  * In most cases, `foreach` improves training speed up to 1.1 ~ 1.5x with a moderate increase in memory usage.
  * Like `Pytorch` official optimizer, the default value of `foreach` is `None`, and if unspecified by the user (so foreach is None), we will try to use foreach over the for-loop implementation on CUDA.

### CI/CD

* Introduce `uv`. (#473)
