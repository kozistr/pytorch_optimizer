## Change Log

### Feature

* Support the `foreach` functionality. (#287, #476, #477)
  * Currently, 10+ optimizers (e.g. AdaFactor, StableAdamW, Lion, AdaBelief, Amos, ...) support `foreach`.
  * In most cases, `foreach` improves training speed up to 1.1 ~ 1.5x with a moderate increase in memory usage.
  * Like `Pytorch` official optimizer, the default value of `foreach` is `None`, and if unspecified by the user (so foreach is None), we will try to use foreach over the for-loop implementation on CUDA.
* Update the Emo-series optimizers. (#472, #478)
  * Update `EmoNavi`, `EmoFact`, and `EmoLynx`. 
  * Fade out `EmoNeco` and `EmoZeal` optimizers.
* Implement `SpectralSphere` optimizer. (#483, #485)
  * [Controlled LLM Training on Spectral Sphere](https://arxiv.org/abs/2601.08393)

### Fix

* misbehavior in `AdaFactor` optimizer. (#477)
* potential `NaN` issue in `AdamP` optimizer. (#480, #481)

### CI/CD

* Introduce `uv`. (#473)
