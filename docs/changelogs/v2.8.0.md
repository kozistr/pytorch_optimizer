## Change Log

### Feature

* Implement A2Grad optimizer (#136)
  * [Optimal Adaptive and Accelerated Stochastic Gradient Descent](https://arxiv.org/abs/1810.00553)
* Implement Accelerated SGD optimizer (#137)
  * [Accelerating Stochastic Gradient Descent For Least Squares Regression](https://arxiv.org/abs/1704.08227)
* Implement Adaptive SGD optimizer (#139)
  * [Adaptive Gradient Descent without Descent](https://arxiv.org/abs/1910.09529)
* Implement SGDW optimizer (#139)
  * [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)
* Implement Yogi optimizer (#140)
  * [Adaptive Methods for Nonconvex Optimization](https://papers.nips.cc/paper_files/paper/2018/hash/90365351ccc7437a1309dc64e4db32a3-Abstract.html)
* Implement SWATS optimizer (#141)
  * [Improving Generalization Performance by Switching from Adam to SGD](https://arxiv.org/abs/1712.07628) 
* Implement Fromage optimizer (#142)
  * [On the distance between two neural networks and the stability of learning](https://arxiv.org/abs/2002.03432) 
* Implement MSVAG optimizer (#143)
  * [Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients](https://arxiv.org/abs/1705.07774) 
* Implement AdaMod optimizer (#144)
  * [An Adaptive and Momental Bound Method for Stochastic Learning](https://arxiv.org/abs/1910.12249) 
* Implement AggMo optimizer (#145)
  * [Aggregated Momentum: Stability Through Passive Damping](https://arxiv.org/abs/1804.00325)
* Implement QHAdam, QHM optimizers (#146)
  * [Quasi-hyperbolic momentum and Adam for deep learning](https://arxiv.org/abs/1810.06801)
* Implement PID optimizer (#147)
  * [A PID Controller Approach for Stochastic Optimization of Deep Networks](http://www4.comp.polyu.edu.hk/~cslzhang/paper/CVPR18_PID.pdf) 

### Bug

* Fix `update` in Lion optimizer (#135)
* Fix `momentum_buffer` in SGDP optimizer (#139)

### Diff

[2.7.0...2.8.0](https://github.com/kozistr/pytorch_optimizer/compare/v2.7.0...v2.8.0)
