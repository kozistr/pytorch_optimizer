## Change Log

## Feature

* Implement more cooldown types for WSD learning rate scheduler. (#382, #386)
* Implement `AdamWSN` optimizer. (#387, #389)
    * [Lean and Mean Adaptive Optimization via Subset-Norm and Subspace-Momentum with Convergence Guarantees](https://arxiv.org/abs/2411.07120)

### Fix

* Fix to use `momentum buffer` instead of the gradient to calculate LMO. (#385)
